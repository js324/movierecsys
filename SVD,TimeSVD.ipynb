{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DO NOT RUN Unit Tests or Testing Blocks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import math\n",
    "from random import sample\n",
    "import os\n",
    "import datetime\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install surprise\n",
    "from surprise import AlgoBase, Dataset, SVD, Reader, accuracy, SVDpp\n",
    "from surprise.model_selection import cross_validate, KFold, train_test_split\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Implementation of SVD algorithm (basic/control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDImpl(AlgoBase):\n",
    "    def __init__(self, learning_rate, n_epochs, n_factors):\n",
    "        \n",
    "        self.lr = learning_rate  # learning rate for SGD\n",
    "        self.n_epochs = n_epochs  # number of iterations of SGD\n",
    "        self.n_factors = n_factors  # number of factors\n",
    "        self.lr_all = .005\n",
    "        self.reg_all = .02\n",
    "        AlgoBase.__init__(self)\n",
    "        \n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "        '''Learn the vectors p_u and q_i with SGD'''\n",
    "        \n",
    "        print('Fitting data with SGD...')\n",
    "        #Initialize the user and item biases\n",
    "        bu = np.zeros(trainset.n_users)\n",
    "        bi = np.zeros(trainset.n_items)\n",
    "        # Randomly initialize the user and item factors.\n",
    "        pu = np.random.normal(0, .1, (trainset.n_users, self.n_factors))\n",
    "        qi = np.random.normal(0, .1, (trainset.n_items, self.n_factors))\n",
    "        lr_bu, lr_bi, lr_pu, lr_qi = self.lr_all, self.lr_all, self.lr_all, self.lr_all\n",
    "        reg_bu, reg_bi, reg_pu, reg_qi = self.reg_all, self.reg_all, self.reg_all, self.reg_all\n",
    "        # SGD procedure\n",
    "        for _ in range(self.n_epochs):\n",
    "            for u, i, r_ui in trainset.all_ratings():\n",
    "                dot = 0  # <q_i, p_u>\n",
    "                for f in range(self.n_factors):\n",
    "                    dot += qi[i, f] * pu[u, f]\n",
    "                err = r_ui - (trainset.global_mean + bu[u] + bi[i] + dot)\n",
    "\n",
    "                # update biases\n",
    "                bu[u] += lr_bu * (err - reg_bu * bu[u])\n",
    "                bi[i] += lr_bi * (err - reg_bi * bi[i])\n",
    "\n",
    "                # update factors\n",
    "                for f in range(self.n_factors):\n",
    "                    puf = pu[u, f]\n",
    "                    qif = qi[i, f]\n",
    "                    pu[u, f] += lr_pu * (err * qif - reg_pu * puf)\n",
    "                    qi[i, f] += lr_qi * (err * puf - reg_qi * qif)\n",
    "        \n",
    "        self.bu = np.asarray(bu)\n",
    "        self.bi = np.asarray(bi)\n",
    "        self.pu = np.asarray(pu)\n",
    "        self.qi = np.asarray(qi)\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        '''Return the estmimated rating of user u for item i.'''\n",
    "        \n",
    "        known_user = self.trainset.knows_user(u)\n",
    "        known_item = self.trainset.knows_item(i)\n",
    "\n",
    "        est = self.trainset.global_mean\n",
    "\n",
    "        if known_user:\n",
    "            est += self.bu[u]\n",
    "\n",
    "        if known_item:\n",
    "            est += self.bi[i]\n",
    "\n",
    "        if known_user and known_item:\n",
    "            est += np.dot(self.qi[i], self.pu[u])\n",
    "        return est"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Creation of Timestamp dictionary and MeanDate dictionary for use within TimeSVD algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r_cols = ['user_id', 'movie_id', 'rating','timeStamp']\n",
    "ratings = pd.read_csv('ratings.csv', header=None,skiprows=1,sep=',', names=r_cols)\n",
    "data = pd.DataFrame(ratings,columns = ['user_id','movie_id','timeStamp'])\n",
    "tsDict = dict()\n",
    "#setting dict to access timestamps of ratings\n",
    "for row in data.iterrows():\n",
    "    tsDict[(row[1]['user_id'], row[1]['movie_id'])] = row[1]['timeStamp']\n",
    "data_ind = data.set_index(['user_id','movie_id'])\n",
    "meanDate= np.nanmean(data_ind)\n",
    "avgtime_byuser = data.groupby(by=data.user_id).timeStamp.mean()\n",
    "avgTimeU = dict()\n",
    "for uid, avgts in avgtime_byuser.items():\n",
    "    avgTimeU[uid] = avgts\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Implementation of timeSVD algorithm (attempted linear modeling with single day effect). It is currently only linear model with the movie-related temporal effects and temporal effects on user biases accurately captured (there are two versions, one with just temporal effects on item bias and one with both item bias and user bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class timeSVDImplI(AlgoBase):\n",
    "    def __init__(self, learning_rate, n_epochs, n_factors):\n",
    "        \n",
    "        self.lr = learning_rate  # learning rate for SGD\n",
    "        self.n_epochs = n_epochs  # number of iterations of SGD\n",
    "        self.n_factors = n_factors  # number of factors\n",
    "        self.lr_all = .005\n",
    "        self.reg_all = .02\n",
    "        self.numBins = 23  # 23 bins yearly from 1996 to 2018\n",
    "        self.meanDate = meanDate\n",
    "        self.B = .00005\n",
    "        AlgoBase.__init__(self)\n",
    "       \n",
    "        \n",
    "        \n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "        '''Learn the vectors p_u and q_i with SGD'''\n",
    "        \n",
    "        print('Fitting data with SGD...')\n",
    "        #Initialize the user and item biases\n",
    "        bu = np.zeros(trainset.n_users, dtype=np.longdouble)\n",
    "        au = np.random.normal(0, .1, (trainset.n_users))\n",
    "        au = np.zeros(trainset.n_users, dtype=np.longdouble)\n",
    "        bi = np.zeros(trainset.n_items, dtype=np.longdouble)\n",
    "        biBin = [np.copy(bi)]*23\n",
    "        # but = [np.copy(bu)]*40\n",
    "        but = [dict()]*trainset.n_users\n",
    "        \n",
    "        # Randomly initialize the user and item factors.\n",
    "        pu = np.random.normal(0, .1, (trainset.n_users, self.n_factors))\n",
    "        qi = np.random.normal(0, .1, (trainset.n_items, self.n_factors))\n",
    "        lr_bu, lr_bi, lr_pu, lr_qi = self.lr_all, self.lr_all, self.lr_all, self.lr_all\n",
    "        reg_bu, reg_bi, reg_pu, reg_qi = self.reg_all, self.reg_all, self.reg_all, self.reg_all\n",
    "        \n",
    "        # SGD procedure\n",
    "        for _ in range(self.n_epochs):\n",
    "            print(\"Going through epoch...\")\n",
    "            for u, i, r_ui in trainset.all_ratings():\n",
    "                \n",
    "                dot = 0  \n",
    "                for f in range(self.n_factors):\n",
    "                    dot += qi[i, f] * pu[u, f]\n",
    "                u_rawid = int(trainset.to_raw_uid(u))\n",
    "                i_rawid = int(trainset.to_raw_iid(i))\n",
    "                \n",
    "        \n",
    "                # diffDaySign = np.sign(currTs-avgTimeU[u_rawid])\n",
    "                # devu_t = diffDaySign * pow(abs((datetime.datetime.fromtimestamp(currTs)- \\\n",
    "                #                        datetime.datetime.fromtimestamp(avgTimeU[u_rawid]))).days, self.B)\n",
    "              \n",
    "                # if currTsF.days not in but[u]:\n",
    "                #     but[u][currTsF.days] = 0\n",
    "                    \n",
    "                err = r_ui - (trainset.global_mean + bu[u] + bi[i] + dot)\n",
    "                                # au[u]*devu_t) # + but[u][currTsF.days])\n",
    "                # needed for when we're predicting with unknown timestamps\n",
    "                if (u_rawid,i_rawid) in tsDict:\n",
    "                    currTs = tsDict[(u_rawid,i_rawid)]\n",
    "                    # currTsF = (datetime.datetime.fromtimestamp(currTs)- datetime.datetime(1996,3,29))\n",
    "                    binNum = datetime.datetime.fromtimestamp(currTs).year-1996\n",
    "                    err -= biBin[binNum][i]\n",
    "                    biBin[binNum][i] += lr_bi * (err - reg_bi * biBin[binNum][i])\n",
    "                # update biases\n",
    "                \n",
    "                # movie bias\n",
    "                #bi(t) = bi + bi,Bin(t)\n",
    "                bi[i] += lr_bi * (err - reg_bi * bi[i]) \n",
    "                \n",
    "                \n",
    "                # user bias\n",
    "                #b u (t) = bu + αu · devu(t) \n",
    "                bu[u] += lr_bu * (err - reg_bu * bu[u])\n",
    "                # au[u] += lr_bu * (err*devu_t - reg_bu * au[u]) # SEE SGD CALCULATION/DERIVATIVE\n",
    "                \n",
    "                # but[u][currTsF.days] = lr_bu * (err - reg_bu * but[u][currTsF.days])\n",
    "                \n",
    "                # update factors\n",
    "                for f in range(self.n_factors):\n",
    "                    puf = pu[u, f]\n",
    "                    qif = qi[i, f]\n",
    "                    pu[u, f] += lr_pu * (err * qif - reg_pu * puf)\n",
    "                    qi[i, f] += lr_qi * (err * puf - reg_qi * qif)\n",
    "        \n",
    "        self.bu = np.asarray(bu)\n",
    "        self.bi = np.asarray(bi)\n",
    "        self.biBin = np.asarray(biBin)\n",
    "        self.but = np.asarray(but)\n",
    "        \n",
    "        self.au = np.asarray(au)\n",
    "        self.pu = np.asarray(pu)\n",
    "        self.qi = np.asarray(qi)\n",
    "        \n",
    "    def estimate(self, u, i):\n",
    "        '''Return the estmimated rating of user u for item i.'''\n",
    "        \n",
    "        known_user = self.trainset.knows_user(u)\n",
    "        known_item = self.trainset.knows_item(i)\n",
    "\n",
    "        est = self.trainset.global_mean\n",
    "        \n",
    "        if known_user:\n",
    "            est += self.bu[u]\n",
    "            \n",
    "\n",
    "        if known_item:\n",
    "            est += self.bi[i]\n",
    "\n",
    "        if known_user and known_item:\n",
    "            u_rawid = int(self.trainset.to_raw_uid(u))\n",
    "            i_rawid = int(self.trainset.to_raw_iid(i))\n",
    "            if (u_rawid,i_rawid) in tsDict:\n",
    "                currTs = tsDict[(u_rawid,i_rawid)]\n",
    "                if currTs == 0:\n",
    "                    print(\"FOUND\")\n",
    "                currTsF = (datetime.datetime.fromtimestamp(currTs)- datetime.datetime(1996,3,29))\n",
    "                binNum = datetime.datetime.fromtimestamp(currTs).year-1996\n",
    "                #BiBin(t)\n",
    "                est += self.biBin[binNum][i]\n",
    "            \n",
    "            #au*devu_t(t)\n",
    "            # diffDaySign = np.sign(currTs-avgTimeU[u_rawid])\n",
    "            # devu_t = diffDaySign * pow(abs((datetime.datetime.fromtimestamp(currTs)- \\\n",
    "            #     datetime.datetime.fromtimestamp(avgTimeU[u_rawid]))).days, self.B)\n",
    "            \n",
    "            # est += self.au[u]*devu_t\n",
    "            \n",
    "            #jank hack, why is currTsF.days not a valid key sometimes?\n",
    "            # if currTsF.days in self.but[u]:\n",
    "            #     est += self.but[u][currTsF.days]\n",
    "            est += np.dot(self.qi[i], self.pu[u])\n",
    "            \n",
    "        return est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class timeSVDImplUI(AlgoBase):\n",
    "    def __init__(self, learning_rate, n_epochs, n_factors):\n",
    "        \n",
    "        self.lr = learning_rate  # learning rate for SGD\n",
    "        self.n_epochs = n_epochs  # number of iterations of SGD\n",
    "        self.n_factors = n_factors  # number of factors\n",
    "        self.lr_all = .005\n",
    "        self.reg_all = .02\n",
    "        self.numBins = 23  # 23 bins yearly from 1996 to 2018\n",
    "        self.meanDate = meanDate\n",
    "        self.B = .00005\n",
    "        AlgoBase.__init__(self)\n",
    "       \n",
    "        \n",
    "        \n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "        '''Learn the vectors p_u and q_i with SGD'''\n",
    "        \n",
    "        print('Fitting data with SGD...')\n",
    "        #Initialize the user and item biases\n",
    "        bu = np.zeros(trainset.n_users, dtype=np.longdouble)\n",
    "        au = np.random.normal(0, .1, (trainset.n_users))\n",
    "        au = np.zeros(trainset.n_users, dtype=np.longdouble)\n",
    "        bi = np.zeros(trainset.n_items, dtype=np.longdouble)\n",
    "        biBin = [np.copy(bi)]*23\n",
    "        # but = [np.copy(bu)]*40\n",
    "        but = [dict()]*trainset.n_users\n",
    "        \n",
    "        # Randomly initialize the user and item factors.\n",
    "        pu = np.random.normal(0, .1, (trainset.n_users, self.n_factors))\n",
    "        qi = np.random.normal(0, .1, (trainset.n_items, self.n_factors))\n",
    "        lr_bu, lr_bi, lr_pu, lr_qi = self.lr_all, self.lr_all, self.lr_all, self.lr_all\n",
    "        reg_bu, reg_bi, reg_pu, reg_qi = self.reg_all, self.reg_all, self.reg_all, self.reg_all\n",
    "        \n",
    "        # SGD procedure\n",
    "        for _ in range(self.n_epochs):\n",
    "            print(\"Going through epoch...\")\n",
    "            for u, i, r_ui in trainset.all_ratings():\n",
    "                \n",
    "                dot = 0  \n",
    "                for f in range(self.n_factors):\n",
    "                    dot += qi[i, f] * pu[u, f]\n",
    "                u_rawid = int(trainset.to_raw_uid(u))\n",
    "                i_rawid = int(trainset.to_raw_iid(i))\n",
    "                \n",
    "              \n",
    "                # if currTsF.days not in but[u]:\n",
    "                    # but[u][currTsF.days] = 0\n",
    "                    \n",
    "                err = r_ui - (trainset.global_mean + bu[u] + bi[i] + dot) # + biBin[binNum][i] + \\\n",
    "                                # au[u]*devu_t) # + but[u][currTsF.days])\n",
    "                \n",
    "                # update biases\n",
    "                if (u_rawid,i_rawid) in tsDict:\n",
    "                    currTs = tsDict[(u_rawid,i_rawid)]\n",
    "                    # currTsF = (datetime.datetime.fromtimestamp(currTs)- datetime.datetime(1996,3,29))\n",
    "                    binNum = datetime.datetime.fromtimestamp(currTs).year-1996\n",
    "                    err -= biBin[binNum][i]\n",
    "                    \n",
    "                    if u_rawid in avgTimeU: \n",
    "                        diffDaySign = np.sign(currTs-avgTimeU[u_rawid])\n",
    "                        devu_t = diffDaySign * pow(abs((datetime.datetime.fromtimestamp(currTs)- \\\n",
    "                                               datetime.datetime.fromtimestamp(avgTimeU[u_rawid]))).days, self.B)\n",
    "                        err -= au[u]*devu_t\n",
    "                        \n",
    "                    biBin[binNum][i] += lr_bi * (err - reg_bi * biBin[binNum][i])\n",
    "                    if u_rawid in avgTimeU:\n",
    "                        au[u] += lr_bu * (err*devu_t - reg_bu * au[u]) # SEE SGD CALCULATION/DERIVATIVE\n",
    "                # movie bias\n",
    "                #bi(t) = bi + bi,Bin(t)\n",
    "                bi[i] += lr_bi * (err - reg_bi * bi[i]) \n",
    "                \n",
    "                \n",
    "                # user bias\n",
    "                #b u (t) = bu + αu · devu(t) \n",
    "                bu[u] += lr_bu * (err - reg_bu * bu[u])\n",
    "                \n",
    "                \n",
    "                # but[u][currTsF.days] = lr_bu * (err - reg_bu * but[u][currTsF.days])\n",
    "                \n",
    "                # update factors\n",
    "                for f in range(self.n_factors):\n",
    "                    puf = pu[u, f]\n",
    "                    qif = qi[i, f]\n",
    "                    pu[u, f] += lr_pu * (err * qif - reg_pu * puf)\n",
    "                    qi[i, f] += lr_qi * (err * puf - reg_qi * qif)\n",
    "        \n",
    "        self.bu = np.asarray(bu)\n",
    "        self.bi = np.asarray(bi)\n",
    "        self.biBin = np.asarray(biBin)\n",
    "        self.but = np.asarray(but)\n",
    "        \n",
    "        self.au = np.asarray(au)\n",
    "        self.pu = np.asarray(pu)\n",
    "        self.qi = np.asarray(qi)\n",
    "        \n",
    "    def estimate(self, u, i):\n",
    "        '''Return the estmimated rating of user u for item i.'''\n",
    "        \n",
    "        known_user = self.trainset.knows_user(u)\n",
    "        known_item = self.trainset.knows_item(i)\n",
    "\n",
    "        est = self.trainset.global_mean\n",
    "        \n",
    "        if known_user:\n",
    "            est += self.bu[u]\n",
    "            \n",
    "\n",
    "        if known_item:\n",
    "            est += self.bi[i]\n",
    "\n",
    "        if known_user and known_item:\n",
    "            u_rawid = int(self.trainset.to_raw_uid(u))\n",
    "            i_rawid = int(self.trainset.to_raw_iid(i))\n",
    "            if (u_rawid,i_rawid) in tsDict:\n",
    "                currTs = tsDict[(u_rawid,i_rawid)]\n",
    "                currTsF = (datetime.datetime.fromtimestamp(currTs)- datetime.datetime(1996,3,29))\n",
    "                binNum = datetime.datetime.fromtimestamp(currTs).year-1996\n",
    "                #BiBin(t)\n",
    "                est += self.biBin[binNum][i]\n",
    "                if u_rawid in avgTimeU:\n",
    "                    #au*devu_t(t)\n",
    "                    diffDaySign = np.sign(currTs-avgTimeU[u_rawid])\n",
    "                    devu_t = diffDaySign * pow(abs((datetime.datetime.fromtimestamp(currTs)- \\\n",
    "                        datetime.datetime.fromtimestamp(avgTimeU[u_rawid]))).days, self.B)\n",
    "                    est += self.au[u]*devu_t\n",
    "            \n",
    "            #jank hack, why is currTsF.days not a valid key sometimes?\n",
    "            # if currTsF.days in self.but[u]:\n",
    "            #     est += self.but[u][currTsF.days]\n",
    "            est += np.dot(self.qi[i], self.pu[u])\n",
    "            \n",
    "        return est"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using surprise to test our algorithms with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the movielens-100k dataset (download it if needed),\n",
    "reader = Reader(line_format=\"user item rating timestamp\", sep=\",\", skip_lines=1)\n",
    "\n",
    "file_path = \"/common/home/js2715/cs550/ratings.csv\"\n",
    "data = Dataset.load_from_file(file_path, reader=reader)\n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "algo = timeSVDImplUI(learning_rate=.01, n_epochs=10, n_factors=10)\n",
    "# algo2 = SVDImpl(learning_rate=.01, n_epochs=10, n_factors=10)\n",
    "# algo2 = SVD()\n",
    "# Run 5-fold cross-validation and print results\n",
    "cross_validate(algo, data, measures=[\"RMSE\", \"MAE\"], cv=2, verbose=True)\n",
    "# cross_validate(algo2, data, measures=[\"RMSE\", \"MAE\"], cv=2, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of new \"proper\" training and testing sets (refer to report for explanation)\n",
    "Basically two fold, sorting by timestamp and taking proportion of ratings from each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r_cols = ['user_id', 'movie_id', 'rating','timeStamp']\n",
    "ratings = pd.read_csv('ratings.csv', header=None,skiprows=1,sep=',', names=r_cols)\n",
    "data = pd.DataFrame(ratings,columns = ['user_id','movie_id', 'rating', 'timeStamp'])\n",
    "sortedData = data.sort_values(['user_id','timeStamp'],ascending=False).groupby('user_id')\n",
    "training_data = pd.DataFrame(columns = ['user_id','movie_id', 'rating', 'timeStamp'])\n",
    "testing_data = pd.DataFrame(columns = ['user_id','movie_id', 'rating', 'timeStamp'])\n",
    "for key, item in sortedData:\n",
    "    cutoff = int(len(item.index)*.2)\n",
    "    df1 = item.iloc[:cutoff,:]\n",
    "    df2 = item.iloc[cutoff:,:]\n",
    "    for ind, row in df2.iterrows():\n",
    "        training_data.loc[len(training_data)] = row\n",
    "    for ind, row in df1.iterrows():\n",
    "        testing_data.loc[len(testing_data)] = row\n",
    "print(\"DONE\")\n",
    "   \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = training_data.drop(columns=['timeStamp'])\n",
    "# testing_data = testing_data.drop(columns=['timeStamp'])\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "train_data = Dataset.load_from_df(training_data[['user_id','movie_id', 'rating']], reader)\n",
    "train_data = train_data.build_full_trainset()\n",
    "test_data = Dataset.load_from_df(testing_data[['user_id','movie_id', 'rating']], reader)\n",
    "\n",
    "new_test = test_data.build_full_trainset().build_testset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "algo = timeSVDImplUI(learning_rate=.01, n_epochs=10, n_factors=10)\n",
    "# algo = SVDImpl(learning_rate=.01, n_epochs=10, n_factors=10)\n",
    "# algo = SVD()\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(train_data)\n",
    "predictions = algo.test(new_test)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)\n",
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all of the top 10 recommendations for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modified from https://github.com/NicolasHug/Surprise/blob/master/examples/top_n_recommendations.py \n",
    "#- Nicholas Hug's (Creator of Surprise library) implementation of top_n_recommendations\n",
    "def get_top_n(predictions, n=10):\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "data = Dataset.load_from_file(file_path, reader=reader)\n",
    "trainset = data.build_full_trainset() \n",
    "algo = SVDImpl(learning_rate=.01, n_epochs=10, n_factors=10) \n",
    "algo.fit(trainset)\n",
    "\n",
    "testset = trainset.build_anti_testset()\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "top_n_SVDImpl = get_top_n(predictions, n=10)\n",
    "\n",
    "# Print the recommended items for each user\n",
    "for uid, user_ratings in top_n.items():\n",
    "    print(uid, [iid for (iid, _) in user_ratings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = Dataset.load_from_file(file_path, reader=reader)\n",
    "trainset = data.build_full_trainset() \n",
    "algo = timeSVDImplI(learning_rate=.01, n_epochs=10, n_factors=10) \n",
    "algo.fit(trainset)\n",
    "\n",
    "testset = trainset.build_anti_testset()\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "top_n_SVDImplI = get_top_n(predictions, n=10)\n",
    "\n",
    "# Print the recommended items for each user\n",
    "print(\"SVDIMPLI TOP RECOMMENDATIONS\")\n",
    "for uid, user_ratings in top_n.items():\n",
    "    print(uid, [iid for (iid, _) in user_ratings])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = Dataset.load_from_file(file_path, reader=reader)\n",
    "trainset = data.build_full_trainset() \n",
    "algo = timeSVDImplUI(learning_rate=.01, n_epochs=10, n_factors=10) \n",
    "algo.fit(trainset)\n",
    "\n",
    "\n",
    "testset = trainset.build_anti_testset()\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "top_n_SVDImplUI = get_top_n(predictions, n=10)\n",
    "\n",
    "# Print the recommended items for each user\n",
    "print(\"SVDIMPLUI TOP RECOMMENDATIONS\")\n",
    "for uid, user_ratings in top_n.items():\n",
    "    print(uid, [iid for (iid, _) in user_ratings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7207170660196139\n",
      "Recall: 0.5455292063587537\n",
      "F-Measure: 0.6210043300603779\n",
      "NDCG: 0.4470021892019521\n"
     ]
    }
   ],
   "source": [
    "#Modified from https://surprise.readthedocs.io/en/stable/FAQ.html#precision-recall-at-k-py \n",
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    \n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    ndcg = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        \n",
    "        dcg = sum((true_r*(1/(math.log2(ind+2)))) for ind, (_, true_r) in enumerate(user_ratings))\n",
    "        dcg_max = sum((true_r*(1)) for ind, (_, true_r) in enumerate(user_ratings))\n",
    "        ndcg[uid] = dcg/dcg_max\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(\n",
    "            ((true_r >= threshold) and (est >= threshold))\n",
    "            for (est, true_r) in user_ratings[:k]\n",
    "        )\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "        \n",
    "    return precisions, recalls, ndcg\n",
    "\n",
    "\n",
    "data = Dataset.load_builtin(\"ml-100k\")\n",
    "\n",
    "# algo = timeSVDImplUI(learning_rate=.01, n_epochs=10, n_factors=10)\n",
    "algo = SVDpp()\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "precisions, recalls, ndcg = precision_recall_at_k(predictions, k=10, threshold=3.5)\n",
    "total_prec = sum(prec for prec in precisions.values()) / len(precisions)\n",
    "total_rec =sum(rec for rec in recalls.values()) / len(recalls)\n",
    "total_ndcg = sum(val for val in ndcg.values()) / len(ndcg)\n",
    "# Precision and recall can then be averaged over all users\n",
    "print(\"Precision:\", total_prec)\n",
    "print(\"Recall:\", total_rec)\n",
    "print(\"F-Measure:\",(2*total_prec*total_rec)/(total_prec+total_rec))\n",
    "print(\"NDCG:\", total_ndcg)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
